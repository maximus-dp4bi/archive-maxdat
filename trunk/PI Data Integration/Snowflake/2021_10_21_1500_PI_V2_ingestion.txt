CREATE or replace PROCEDURE createProjectFilesListTables (reIngestFlag Boolean)
  RETURNS VARCHAR
  LANGUAGE javascript
  EXECUTE AS CALLER
  AS
  $$
  
  //set logging on
     var sqlCommand = "set do_log = true;";
     snowflake.execute ({sqlText: sqlCommand});
     sqlCommand = "set log_table = 'INGEST_PI_DATA_DET_LOG';";
     snowflake.execute ({sqlText: sqlCommand});
 
function log(projectid, object_category, object_name, status_string, msg){
    snowflake.createStatement( { sqlText: `call do_log_det(:1, :2, :3, :4, :5)`, binds:[projectid, object_category, object_name, status_string, msg] } ).execute();
} 
       

function convertTZ(date, tzString) {
    return new Date((typeof date === "string" ? new Date(date) : date).toLocaleString("en-US", {timeZone: tzString}));   
}

     //get date string for today
     var today_local = new Date();
     var today = convertTZ(today_local,'America/New_York')
     var dd = String(today.getDate()).padStart(2, '0');
     var mm = String(today.getMonth() + 1).padStart(2, '0'); //January is 0!
     var yyyy = today.getFullYear();
     var hh = String(today.getHours());
     var todayDateString = yyyy + mm + dd;

REINGEST_SFX = ""
if (REINGESTFLAG){
REINGEST_SFX=" REINGEST";
}         

var project_array=[];
var sqlCommand = "select projectid from public.d_pi_projects where active=true;"
var createdSQL = snowflake.createStatement( { sqlText: sqlCommand } );
    var rs = createdSQL.execute();
    while (rs.next()) {
     var projectid = rs.getColumnValue(1);
     project_array.push(projectid);
    }
  
  for (const projectid of project_array) {
//create tables
    sqlCommand = `create or replace TABLE PI_FILES_TO_INGEST_`+ projectid +` (
	PROJECTID VARCHAR(255),
	PROGRAMID VARCHAR(255),
    TABLENAME VARCHAR(255),
	FILENAME VARCHAR(255),
  	FILE_TIMESTAMP TIMESTAMP_TZ(9),
	INGEST_TIMESTAMP TIMESTAMP_TZ(9)
);`;
    
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(projectid, "LOG"+REINGEST_SFX, "NA", "FAILED","Failed to create ingest files list table for project " + projectid +  ": "+ err); 
        return "3"; 
}

}     

    return "0";  
  $$;
  
CREATE or replace PROCEDURE createPIStageV2(AWSFolderName VARCHAR, stageName VARCHAR)
  RETURNS VARCHAR
  LANGUAGE javascript
  EXECUTE AS CALLER
  AS
  $$
     
     //default stageName
     var stageName = STAGENAME;
     if (! stageName){
        stageName = 'PUREINSIGHTS_S3_2';
     }     
     
    //create stage for today's data
    var sqlCommand = "CREATE STAGE RAW." + stageName + " URL = 's3://soapax-pureinsights-v2/"+ AWSFOLDERNAME +"/' CREDENTIALS=(aws_key_id='AKIAT6QV4CSZF5RW77X4' aws_secret_key='secret key goes here');";
    snowflake.execute ({sqlText: sqlCommand});

  return "0";
  $$;  
  
--'2021/09/15 00:00:00'
call getFilesToIngest ('701',null,'transcription_program_configuration','PI_V2_TOPIC_PHRASES','2021/09/08 00:00:00',false);

  CREATE or replace PROCEDURE getFilesToIngest(projectid varchar, programName varchar, tableName varchar, stageName varchar, afterTime varchar, beforeTime varchar, reIngestFlag Boolean)
  RETURNS VARCHAR
  LANGUAGE javascript
  EXECUTE AS CALLER
  AS
  $$
  
  //set logging on
     var sqlCommand = "set do_log = true;";
     snowflake.execute ({sqlText: sqlCommand});
     sqlCommand = "set log_table = 'INGEST_PI_DATA_DET_LOG';";
     snowflake.execute ({sqlText: sqlCommand});
 
function log(projectid, object_category, object_name, status_string, msg){
    snowflake.createStatement( { sqlText: `call do_log_det(:1, :2, :3, :4, :5)`, binds:[projectid, object_category, object_name, status_string, msg] } ).execute();
} 
       

function convertTZ(date, tzString) {
    return new Date((typeof date === "string" ? new Date(date) : date).toLocaleString("en-US", {timeZone: tzString}));   
}

     //get date string for today
     var today_local = new Date();
     var today = convertTZ(today_local,'America/New_York')
     var dd = String(today.getDate()).padStart(2, '0');
     var mm = String(today.getMonth() + 1).padStart(2, '0'); //January is 0!
     var yyyy = today.getFullYear();
     var hh = String(today.getHours());
     var todayDateString = yyyy + mm + dd;

REINGEST_SFX = ""
if (REINGESTFLAG){
REINGEST_SFX=" REINGEST";
}         

    sqlCommand = "list @"+ STAGENAME + "/"+ TABLENAME + " pattern='.*/*parquet';";
        if (TABLENAME == "ALL"){
        sqlCommand = "list @"+ STAGENAME + " pattern='.*/*parquet';";
    }
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log( PROJECTID, "S3 DATA"+REINGEST_SFX, "LIST FILES", "FAILED", "Failed to list files for projectid " + PROJECTID + " and table "+TABLENAME+": "+ err); 
        return "4";
        }


sqlCommand = "insert into pi_files_to_ingest_"+PROJECTID+" select * from (with parquet as (select case when split_part(\"name\",'/','-3') in ('transcription_topics','transcription_sentiment') then split_part(\"name\",'/','-3') else split_part(\"name\",'/','-2') end as tablename,split_part(\"name\",'/','-1') as filename,\"last_modified\" as modified_timestamp FROM table(result_scan(last_query_id())) lsid where convert_timezone('UTC','America/New_York',to_timestamp(rtrim(\"last_modified\",' GMT'),'DY, DD MON YYYY HH24:MI:SS')) >= convert_timezone('America/New_York','UTC',to_timestamp('"+AFTERTIME+"','YYYY/MM/DD HH24:MI:SS')) and convert_timezone('UTC','America/New_York',to_timestamp(rtrim(\"last_modified\",' GMT'),'DY, DD MON YYYY HH24:MI:SS')) < convert_timezone('America/New_York','UTC',to_timestamp('"+BEFORETIME+"','YYYY/MM/DD HH24:MI:SS')) order by \"last_modified\") select '"+PROJECTID+"',null, pt.tablename,pt.filename,convert_timezone('UTC','America/New_York',to_timestamp(rtrim(modified_timestamp,' GMT'),'DY, DD MON YYYY HH24:MI:SS')),null from parquet pt left outer join pi_files_to_ingest_"+PROJECTID+" fti on pt.tablename = fti.tablename and pt.filename = fti.filename where ( fti.file_timestamp is null or  convert_timezone('UTC','America/New_York',to_timestamp(rtrim(modified_timestamp,' GMT'),'DY, DD MON YYYY HH24:MI:SS')) > fti.file_timestamp)) ;";
 
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log( PROJECTID, "S3 DATA"+REINGEST_SFX, "LIST FILES", "FAILED", "Failed to list files for projectid " + PROJECTID + " and table "+TABLENAME+": "+ err); 
        return "4";
        }
    return "0";  
  $$;

   CREATE or replace PROCEDURE INGESTPIDATA_V2 (TABLENAME VARCHAR, STAGENAME VARCHAR, AWSFOLDERNAME VARCHAR,PROJECTID VARCHAR,PROJECTNAME VARCHAR, TRUNCATEFLAG BOOLEAN, REINGESTFLAG BOOLEAN,runFromBashFlag Boolean, tableIngestType varchar,tableKeyString varchar)
  RETURNS VARCHAR
  LANGUAGE javascript
  EXECUTE AS CALLER
  AS
  $$  
  
  REINGEST_SFX = ""
if (REINGESTFLAG){
REINGEST_SFX=" REINGEST";
} 
    //set logging on
     var sqlCommand = "set do_log = true;";
     snowflake.execute ({sqlText: sqlCommand});
     sqlCommand = "set log_table = 'INGEST_PI_DATA_DET_LOG';";
     if (RUNFROMBASHFLAG) {
      sqlCommand = "set log_table = 'INGEST_PI_DATA_DET_LOG_"+PROJECTID+"';";
     }
     snowflake.execute ({sqlText: sqlCommand});
     
function log(projectid, object_category, object_name, status_string, msg){
    snowflake.createStatement( { sqlText: `call do_log_det(:1, :2, :3, :4, :5)`, binds:[projectid, object_category, object_name, status_string, msg] } ).execute();
}

     //default stage
     var stageName = STAGENAME;
     if (! stageName){
        stageName = 'PUREINSIGHTS_S3_2';
     } 

var LOG_CATEGORY="TABLE";
if (REINGESTFLAG){
   LOG_CATEGORY="TABLE REINGEST";
}

//get list of files to ingest for this table
var file_array=[];
 sqlCommand = "select filename, to_varchar(file_timestamp) from pi_files_to_ingest_" + PROJECTID + " where projectid = '"+PROJECTID+"' and tablename = '"+TABLENAME+"' and ingest_timestamp is null ;";
    var createdSQL = snowflake.createStatement( { sqlText: sqlCommand } );
    var rs = createdSQL.execute();
    while (rs.next()) {
     var fileName = rs.getColumnValue(1);
     var file_ts = rs.getColumnValue(2)
     file_array.push(fileName+";"+file_ts);
    }

 var filesToIngestCount = file_array.length;
 
 if (filesToIngestCount < 1){
  log(PROJECTID,"TABLE"+REINGEST_SFX,TABLENAME,"WARNING","Table " + TABLENAME + " has 0 files to ingest."); 

 } else {
 log(PROJECTID,"TABLE"+REINGEST_SFX,TABLENAME,"INFO","filesToIngestCount: " + filesToIngestCount); 
 
//tableingesttype
//"truncate_insert","merge_no_deletes","merge_with_deletes", "insert"
 log(PROJECTID,"SQL"+REINGEST_SFX,TABLENAME,"INFO","table ingest type: " + TABLEINGESTTYPE); 
if (TABLEINGESTTYPE == "merge_no_deletes"){

//get keys
//var keyString = "conversationId";
var keyCounter = 0;
var keyOnSQL = "";
var selectKeySQL = "";
var keyArray = TABLEKEYSTRING.split(";");
for (const pKey of keyArray){
keyCounter++;
if (keyCounter == 1){
 selectKeySQL = `SELECT parse_json(t.$1):`+pKey+` as pKey`+keyCounter+`,`;
 keyOnSQL = `ON ` +TABLENAME+`.raw:`+pKey+` = bar.pKey`+keyCounter;
 } else if (keyCounter == keyArray.length - 1){
  selectKeySQL += `parse_json(t.$1):`+pKey+` as pKey`+keyCounter+`,`;
   keyOnSQL += ` and ` +TABLENAME+`.raw:`+pKey+` = bar.pKey`+keyCounter;
 } else {
 selectKeySQL += `parse_json(t.$1):`+pKey+` as pKey`+keyCounter+`,`;
 keyOnSQL += ` and ` +TABLENAME+`.raw:`+pKey+` = bar.pKey`+keyCounter;
 }
}


//ingest files

for (const fileinfo of file_array) {
var info_array = fileinfo.split(";");
var filename = info_array[0];
var file_timestamp = info_array[1];

sqlCommand = `MERGE INTO `+TABLENAME+` USING
(
 `+selectKeySQL+`
 t.$1 as newRecord
from @`+stageName+`/`+TABLENAME+`/`+filename+` (PATTERN=>'.*.parquet', FILE_FORMAT => myformat2) t 
) bar `+keyOnSQL+`
and `+TABLENAME+`.projectid = '`+PROJECTID+`'
WHEN MATCHED THEN
 UPDATE SET `+TABLENAME+`.raw = bar.newRecord,`+TABLENAME+`.ingestiondatetime = convert_timezone('UTC','America/New_York',sysdate())
 WHEN NOT MATCHED THEN
 INSERT
 (projectid, projectname, programid, programname,raw, ingestiondatetime
 ) VALUES
 ('`+PROJECTID+`','`+PROJECTNAME+`',null,null,bar.newRecord,convert_timezone('UTC','America/New_York',sysdate())
 );`;
 //log(PROJECTID,"SQL"+REINGEST_SFX,TABLENAME,"INFO","SQL: " + sqlCommand); 

  try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID,LOG_CATEGORY, TABLENAME, "FAILED","Failed to ingest file "+filename+" for table " + TABLENAME +" : "+ err);
        return "2";
        }        

//update files to ingest ingest_ts
   sqlCommand = "update raw.pi_files_to_ingest_"+PROJECTID+" set ingest_timestamp = convert_timezone('UTC','America/New_York',sysdate()) where tablename = '" + TABLENAME + "' and filename = '"+filename+"' and file_timestamp = to_timestamp('" + file_timestamp + "') and ingest_timestamp is null;";
    //log(sqlCommand);
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID, LOG_CATEGORY, TABLENAME, "FAILED", "Failed to update files_to_ingest table for file " + filename+ "for table " +TABLENAME+ ": "+ err); 
        return "3";
        }
  
  }
  
}

if (TABLEINGESTTYPE == "truncate_insert"){

//TABLENAME = TABLENAME + "_nope";
    //truncate table
   if (TRUNCATEFLAG) {
   sqlCommand = "delete from raw."+TABLENAME+" where projectid = '" + PROJECTID + "';";
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID, LOG_CATEGORY, TABLENAME, "FAILED", "Failed to truncate table " +TABLENAME+ " for projectid " + PROJECTID + ": "+ err); 
        return "1";
        }
   }     



    //ingest data from parquet files

for (const fileinfo of file_array) {
var info_array = fileinfo.split(";");
var filename = info_array[0];
var file_timestamp = info_array[1];
    var sqlCommand = "copy into raw." + TABLENAME + " (projectid, projectname, ingestionDateTime, raw) from (select '"+ PROJECTID +"','" + PROJECTNAME + "',convert_timezone('UTC','America/New_York',sysdate()), * from @" + stageName + "/" + TABLENAME + "/) PATTERN='.*"+filename+"' FILE_FORMAT = (TYPE= 'PARQUET') FORCE=TRUE;";
 
 //log(PROJECTID,LOG_CATEGORY, TABLENAME, "INFO",sqlCommand);

try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID,LOG_CATEGORY, TABLENAME, "FAILED","Failed to ingest file "+filename+" for table " + TABLENAME +" : "+ err);
        return "2";
        }        

//update files to ingest ingest_ts
   sqlCommand = "update raw.pi_files_to_ingest_"+PROJECTID+" set ingest_timestamp = convert_timezone('UTC','America/New_York',sysdate()) where tablename = '" + TABLENAME + "' and filename = '"+filename+"' and file_timestamp = to_timestamp('" + file_timestamp + "') and ingest_timestamp is null;";
    //log(sqlCommand);
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID, LOG_CATEGORY, TABLENAME, "FAILED", "Failed to update files_to_ingest table for file " + filename+ "for table " +TABLENAME+ ": "+ err); 
        return "3";
        }
   } 

}
}
     //generate warning if 0 records for any table not listed in unavailable tables table
    sqlCommand = "select count(*) from raw." + TABLENAME + " rw where rw.projectid = '" + PROJECTID + "';"; 
 
     try {
            stmt = snowflake.createStatement({sqlText: sqlCommand});
            var result1 = stmt.execute();
            result1.next();
            rowCount = result1.getColumnValue(1); 
         }
     catch (err)  {
        log(PROJECTID, "TABLE ROW COUNT"+REINGEST_SFX, TABLENAME, "FAILED","Failed to count records after ingestion: "+ err);
        return "4";
        }

        log(PROJECTID,"TABLE"+REINGEST_SFX,TABLENAME,"INFO","rowcount: " + rowCount); 
        
     if (rowCount < 1) {
       log(PROJECTID,"TABLE"+REINGEST_SFX,TABLENAME,"WARNING","Table " + TABLENAME + " has 0 records after ingestion."); 
} 


        log(PROJECTID, LOG_CATEGORY, TABLENAME, "SUCCEEDED","Succeeded loading table " + TABLENAME + " for projectid " + PROJECTID);
  return "0";  
  $$;   
  
  ----------------------------
   CREATE or replace PROCEDURE INGESTPIDATASEGMENTS_V2 (TABLENAME VARCHAR, STAGENAME VARCHAR, AWSFOLDERNAME VARCHAR,PROJECTID VARCHAR,PROJECTNAME VARCHAR, TRUNCATEFLAG BOOLEAN, REINGESTFLAG BOOLEAN,runFromBashFlag Boolean, tableIngestType varchar,tableKeyString varchar)
  RETURNS VARCHAR
  LANGUAGE javascript
  EXECUTE AS CALLER
  AS
  $$  
  
  REINGEST_SFX = ""
if (REINGESTFLAG){
REINGEST_SFX=" REINGEST";
} 
    //set logging on
     var sqlCommand = "set do_log = true;";
     snowflake.execute ({sqlText: sqlCommand});
     sqlCommand = "set log_table = 'INGEST_PI_DATA_DET_LOG';";
     if (RUNFROMBASHFLAG) {
      sqlCommand = "set log_table = 'INGEST_PI_DATA_DET_LOG_"+PROJECTID+"';";
     }
     snowflake.execute ({sqlText: sqlCommand});
     
function log(projectid, object_category, object_name, status_string, msg){
    snowflake.createStatement( { sqlText: `call do_log_det(:1, :2, :3, :4, :5)`, binds:[projectid, object_category, object_name, status_string, msg] } ).execute();
}

     //default stage
     var stageName = STAGENAME;
     if (! stageName){
        stageName = 'PUREINSIGHTS_S3_2';
     } 

var LOG_CATEGORY="TABLE";
if (REINGESTFLAG){
   LOG_CATEGORY="TABLE REINGEST";
}

//get list of files to ingest for this table
var file_array=[];
 sqlCommand = "select filename, to_varchar(file_timestamp) from pi_files_to_ingest_" + PROJECTID + " where projectid = '"+PROJECTID+"' and tablename = '"+TABLENAME+"' and ingest_timestamp is null ;";
    var createdSQL = snowflake.createStatement( { sqlText: sqlCommand } );
    var rs = createdSQL.execute();
    while (rs.next()) {
     var fileName = rs.getColumnValue(1);
     var file_ts = rs.getColumnValue(2)
     file_array.push(fileName+";"+file_ts);
    }

 var filesToIngestCount = file_array.length;
 
 if (filesToIngestCount < 1){
  log(PROJECTID,"TABLE"+REINGEST_SFX,TABLENAME,"WARNING","Table " + TABLENAME + " has 0 files to ingest."); 

 } else {
 log(PROJECTID,"TABLE"+REINGEST_SFX,TABLENAME,"INFO","filesToIngestCount: " + filesToIngestCount); 
 
//tableingesttype
//"truncate_insert","merge_no_deletes","merge_with_deletes", "insert"
 log(PROJECTID,"SQL"+REINGEST_SFX,TABLENAME,"INFO","table ingest type: " + TABLEINGESTTYPE); 
if (TABLEINGESTTYPE == "merge_no_deletes"){

//get keys
//var keyString = "conversationId";
var keyCounter = 0;
var keyOnSQL = "";
var selectKeySQL = "";
var keyArray = TABLEKEYSTRING.split(";");
for (const pKey of keyArray){
keyCounter++;
if (keyCounter == 1){
 selectKeySQL = `SELECT parse_json(t.$1):`+pKey+` as pKey`+keyCounter+`,`;
 keyOnSQL = `ON ` +TABLENAME+`.raw:`+pKey+` = bar.pKey`+keyCounter;
 } else if (keyCounter == keyArray.length - 1){
  selectKeySQL += `parse_json(t.$1):`+pKey+` as pKey`+keyCounter+`,`;
   keyOnSQL += ` and ` +TABLENAME+`.raw:`+pKey+` = bar.pKey`+keyCounter;
 } else {
 selectKeySQL += `parse_json(t.$1):`+pKey+` as pKey`+keyCounter+`,`;
 keyOnSQL += ` and ` +TABLENAME+`.raw:`+pKey+` = bar.pKey`+keyCounter;
 }
}


//ingest files

for (const fileinfo of file_array) {
var info_array = fileinfo.split(";");
var filename = info_array[0];
var file_timestamp = info_array[1];

sqlCommand = `MERGE INTO `+TABLENAME+` USING
(
 `+selectKeySQL+` parse_json(cast('{' || 'errorCode:"'|| $1:errorCode || '",' || 'audioMuted:"'|| $1:audioMuted || '",' || 'conference:"'|| $1:conference || '",' || 'conversationId:"'|| $1:conversationId || '",' || 'disconnecttype:"'|| $1:disconnecttype || '",' || 'participantid:"'|| $1:participantid || '",' || 'queueId:"'|| $1:queueId || '",' || 'segmentduration:"'|| $1:segmentduration || '",' || 'segmentEndTime:"'|| $1:segmentEndTime || '",' || 'segmentStartTime:"'|| $1:segmentStartTime || '",' || 'segmenttype:"'|| $1:segmenttype || '",' || 'sessionId:"'|| $1:sessionId || '",' || 'subject:"'|| $1:subject || '",' || 'wrapupCode:"' || $1:wrapupCode || '"}' as variant)) as newRecord
from @`+stageName+`/`+TABLENAME+`/`+filename+` (PATTERN=>'.*.parquet', FILE_FORMAT => myformat2) t 
) bar `+keyOnSQL+`
and `+TABLENAME+`.projectid = '`+PROJECTID+`'
WHEN MATCHED THEN
 UPDATE SET `+TABLENAME+`.raw = bar.newRecord,`+TABLENAME+`.ingestiondatetime = convert_timezone('UTC','America/New_York',sysdate())
 WHEN NOT MATCHED THEN
 INSERT
 (projectid, projectname, programid, programname,raw, ingestiondatetime
 ) VALUES
 ('`+PROJECTID+`','`+PROJECTNAME+`',null,null,bar.newRecord,convert_timezone('UTC','America/New_York',sysdate())
 );`;
 //log(PROJECTID,"SQL"+REINGEST_SFX,TABLENAME,"INFO","SQL: " + sqlCommand); 

  try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID,LOG_CATEGORY, TABLENAME, "FAILED","Failed to ingest file "+filename+" for table " + TABLENAME +" : "+ err);
        return "2";
        }        

//update files to ingest ingest_ts
   sqlCommand = "update raw.pi_files_to_ingest_"+PROJECTID+" set ingest_timestamp = convert_timezone('UTC','America/New_York',sysdate()) where tablename = '" + TABLENAME + "' and filename = '"+filename+"' and file_timestamp = to_timestamp('" + file_timestamp + "') and ingest_timestamp is null;";
    //log(sqlCommand);
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID, LOG_CATEGORY, TABLENAME, "FAILED", "Failed to update files_to_ingest table for file " + filename+ "for table " +TABLENAME+ ": "+ err); 
        return "3";
        }
  
  }
  
}

if (TABLEINGESTTYPE == "truncate_insert"){

//TABLENAME = TABLENAME + "_nope";
    //truncate table
   if (TRUNCATEFLAG) {
   sqlCommand = "delete from raw."+TABLENAME+" where projectid = '" + PROJECTID + "';";
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID, LOG_CATEGORY, TABLENAME, "FAILED", "Failed to truncate table " +TABLENAME+ " for projectid " + PROJECTID + ": "+ err); 
        return "1";
        }
   }     



    //ingest data from parquet files

for (const fileinfo of file_array) {
var info_array = fileinfo.split(";");
var filename = info_array[0];
var file_timestamp = info_array[1];
    var sqlCommand = "copy into raw." + TABLENAME + " (projectid, projectname, ingestionDateTime, raw) from (select '"+ PROJECTID +"','" + PROJECTNAME + "',convert_timezone('UTC','America/New_York',sysdate()), * from @" + stageName + "/" + TABLENAME + "/) PATTERN='.*"+filename+"' FILE_FORMAT = (TYPE= 'PARQUET') FORCE=TRUE;";
 
 //log(PROJECTID,LOG_CATEGORY, TABLENAME, "INFO",sqlCommand);

try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID,LOG_CATEGORY, TABLENAME, "FAILED","Failed to ingest file "+filename+" for table " + TABLENAME +" : "+ err);
        return "2";
        }        

//update files to ingest ingest_ts
   sqlCommand = "update raw.pi_files_to_ingest_"+PROJECTID+" set ingest_timestamp = convert_timezone('UTC','America/New_York',sysdate()) where tablename = '" + TABLENAME + "' and filename = '"+filename+"' and file_timestamp = to_timestamp('" + file_timestamp + "') and ingest_timestamp is null;";
    //log(sqlCommand);
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID, LOG_CATEGORY, TABLENAME, "FAILED", "Failed to update files_to_ingest table for file " + filename+ "for table " +TABLENAME+ ": "+ err); 
        return "3";
        }
   } 

}
}
     //generate warning if 0 records for any table not listed in unavailable tables table
    sqlCommand = "select count(*) from raw." + TABLENAME + " rw where rw.projectid = '" + PROJECTID + "';"; 
 
     try {
            stmt = snowflake.createStatement({sqlText: sqlCommand});
            var result1 = stmt.execute();
            result1.next();
            rowCount = result1.getColumnValue(1); 
         }
     catch (err)  {
        log(PROJECTID, "TABLE ROW COUNT"+REINGEST_SFX, TABLENAME, "FAILED","Failed to count records after ingestion: "+ err);
        return "4";
        }

        log(PROJECTID,"TABLE"+REINGEST_SFX,TABLENAME,"INFO","rowcount: " + rowCount); 
        
     if (rowCount < 1) {
       log(PROJECTID,"TABLE"+REINGEST_SFX,TABLENAME,"WARNING","Table " + TABLENAME + " has 0 records after ingestion."); 
} 


        log(PROJECTID, LOG_CATEGORY, TABLENAME, "SUCCEEDED","Succeeded loading table " + TABLENAME + " for projectid " + PROJECTID);
  return "0";  
  $$;            
  
  
  -----------------------------

 
CREATE or replace PROCEDURE ingestUningestedPIData_V2(ProjectId VARCHAR,reIngestFlag Boolean, ingestAllFlag Boolean,waitForS3Flag Boolean, runFromBashFlag Boolean)
  RETURNS VARCHAR
  LANGUAGE javascript
  EXECUTE AS CALLER
  AS
  $$
  
  //set logging on
     var sqlCommand = "set do_log = true;";
     snowflake.execute ({sqlText: sqlCommand});
     sqlCommand = "set log_table = 'INGEST_PI_DATA_DET_LOG';";
     if (RUNFROMBASHFLAG) {
      sqlCommand = "set log_table = 'INGEST_PI_DATA_DET_LOG_"+PROJECTID+"';";
     }
     snowflake.execute ({sqlText: sqlCommand});
 
function log(projectid, object_category, object_name, status_string, msg){
    snowflake.createStatement( { sqlText: `call do_log_det(:1, :2, :3, :4, :5)`, binds:[projectid, object_category, object_name, status_string, msg] } ).execute();
} 
       

function convertTZ(date, tzString) {
    return new Date((typeof date === "string" ? new Date(date) : date).toLocaleString("en-US", {timeZone: tzString}));   
}

     //get date string for today
     var today_local = new Date();
     var today = convertTZ(today_local,'America/New_York')
     var dd = String(today.getDate()).padStart(2, '0');
     var dd_prev = String(today.getDate()-1).padStart(2, '0');
     var mm = String(today.getMonth() + 1).padStart(2, '0'); //January is 0!
     var yyyy = today.getFullYear();
     var hh = String(today.getHours());
     var todayDateString = yyyy + mm + dd;
  
  //for testing
     dd = String(today.getDate()+1).padStart(2, '0');
     dd_prev = String(today.getDate()-1).padStart(2, '0');  
  
  var afterTimeString = yyyy + "/" + mm + "/" + dd_prev + " 00:00:00";  
     var beforeTimeString = yyyy + "/" + mm + "/" + dd + " 00:00:00"; 
       log(PROJECTID, "DATE", "NA", "SUCCEEDED",beforeTimeString);
  
REINGEST_SFX = ""
if (REINGESTFLAG){
REINGEST_SFX=" REINGEST";
}     
         
    
 //log start
 if (! RUNFROMBASHFLAG){
     log(PROJECTID, "INGESTION STARTED"+REINGEST_SFX, "NA", "SUCCEEDED","Starting ingestion for PI data");
 }
 
//get project fields 
    var PROJECTNAME = "";
    var AWSFOLDERNAME = "";
    var ingest_wh = "";
    var stageName = "PI_V2_" + PROJECTID;
    var truncateFlag = true;
    var sqlCommand = "select pr.projectname, pr.awsfoldername, pr.ingest_wh from public.d_pi_projects pr where pr.projectid = '"+PROJECTID+"';";
    var stmt = snowflake.createStatement({sqlText: sqlCommand});    
     try {
            var result1 = stmt.execute();
            result1.next();
            PROJECTNAME = result1.getColumnValue(1);
            AWSFOLDERNAME = result1.getColumnValue(2);
            ingest_wh = result1.getColumnValue(3);
         }
    catch (err)  {
        log(PROJECTID, "PROJECT INFO"+REINGEST_SFX, "NA", "FAILED","Failed to get project info from project table: " + err);
        return "2";
        }    
   
   //set ingestion warehouse
     var sqlCommand = "USE WAREHOUSE "+ingest_wh +";";
     try {
     snowflake.execute ({sqlText: sqlCommand});
     } 
     catch(err){
      log(PROJECTID, "WAREHOUSE"+REINGEST_SFX, "NA", "FAILED","Failed to set project ingestion warehouse: " + err + sqlCommand);
     }
   
    //drop stage if still exists
    var blah = 0;
    sqlCommand = "call dropPIStage('" + stageName + "');";
    try { 
    snowflake.execute ({sqlText: sqlCommand}); 
    }
    catch (err){ 
      blah=1;
    }

       
    //create stage for today's data
    sqlCommand = "call createPIStageV2('" + AWSFOLDERNAME + "','" + stageName + "');";
    
    try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID, "STAGE"+REINGEST_SFX, stageName, "FAILED","Failed to create stage" + stageName + " for " +AWSFOLDERNAME+ + " for "+todayDateString+ ": "+ err); 
        return "3"; 
}
  

     
//check and wait for S3 files to be available
//do we still need something like this in V2
   
//how do we deal with unavailable tables in V2?  Just a warning if no files available updaged as of x time ?



  
  //get list of tables yet to be ingested today
  
    var table_array=[];
    if (INGESTALLFLAG){
       sqlCommand = "select ta.table_name,ta.ingest_type, ta.key_string from public.d_pi_projects pr left outer join public.d_pi_tables ta where pr.projectid = '"+PROJECTID+"' and ta.s3_sourced = true and ta.active = true order by ta.table_name; "; 
    } else {
       sqlCommand = "select ta.table_name,ta.ingest_type,ta.key_string from public.d_pi_projects pr left outer join public.d_pi_tables ta where pr.projectid = '"+PROJECTID+"' and ta.s3_sourced = true and ta.active = true and not exists (select 1 from raw.ingest_pi_data_det_log dl where dl.projectid = pr.projectid and dl.object_category in ('TABLE','TABLE REINGEST') and dl.object_name = ta.table_name and dl.status_string = 'SUCCEEDED' and date(convert_timezone('UTC','America/New_York',to_timestamp(to_varchar(dl.ts)))) = date(current_timestamp())) order by ta.table_name; "; 
    }
    
    var createdSQL = snowflake.createStatement( { sqlText: sqlCommand } );
    var rs = createdSQL.execute();
    while (rs.next()) {
     var table_name = rs.getColumnValue(1);
     var table_type = rs.getColumnValue(2);
     var key_string = rs.getColumnValue(3);
     table_array.push(table_name+"|"+table_type+"|"+key_string);
    }
 
 //get list of files to ingest by table
 sqlCommand="call getFilesToIngest ('"+PROJECTID+"',null,'ALL','"+stageName+"','"+afterTimeString+"','"+beforeTimeString+"',"+REINGESTFLAG+");"; 
       log(PROJECTID, "SQL2"+REINGEST_SFX, "NA", "SUCCEEDED",sqlCommand);
  try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID, "FILES"+REINGEST_SFX, "NA", "FAILED","Failed to list files to be ingested" + err); 
        return "5"; 
}
 
  //ingest tables in list
  var rowCount=0;
  var stmt="";
  for (const table_info of table_array) {
  var info_array = table_info.split("|");
  var table_name = info_array[0];
  var tableIngestType = info_array[1];
  var tableKeyString = info_array[2];
     if (table_name != 'segments'){
        sqlCommand = "call ingestPIData_V2('"+ table_name +"','" + stageName + "','" + AWSFOLDERNAME + "','" + PROJECTID + "','" + PROJECTNAME + "',"+ truncateFlag +"," + REINGESTFLAG + ","+RUNFROMBASHFLAG+",'"+tableIngestType+"','"+tableKeyString+"');";
     } else {
       sqlCommand = "call ingestPIDataSegments_V2('"+ table_name +"','" + stageName + "','" + AWSFOLDERNAME + "','" + PROJECTID + "','" + PROJECTNAME + "',"+ truncateFlag +"," + REINGESTFLAG + ","+RUNFROMBASHFLAG+",'"+tableIngestType+"','"+tableKeyString+"');";
     }
     snowflake.execute ({sqlText: sqlCommand});
  }
      
//call sps
  //get list of sps
    var sp_array=[];
    sqlCommand = "select sp.sp_name,sp.schema_name from public.d_pi_ingestion_sps sp order by sp.sp_name; "; 
    var createdSQL = snowflake.createStatement( { sqlText: sqlCommand } );
    var rs = createdSQL.execute();
    while (rs.next()) {
     var sp_name = rs.getColumnValue(1);
     var schema_name = rs.getColumnValue(2)
     sp_array.push(schema_name + "." + sp_name);
    }
  
  //call sps in list
  var rowCount=0;
  var stmt="";
  for (const sp of sp_array) {
     sqlCommand = "call "+ sp + "('"+PROJECTID+"','"+REINGESTFLAG+"',"+RUNFROMBASHFLAG+");"
     snowflake.execute ({sqlText: sqlCommand});
  }    
             
    //drop stage
    sqlCommand = "call dropPIStage('" + stageName + "');";
      try {
            snowflake.execute (
                {sqlText: sqlCommand}
            );
        }
    catch (err)  {
        log(PROJECTID, "STAGE"+REINGEST_SFX, stageName, "FAILED","Failed to drop stage" + stageName + " for " +AWSFOLDERNAME+ + " for "+todayDateString+ ": "+ err); 
        return "5"; 
}

    log(PROJECTID, "INGESTION COMPLETED"+REINGEST_SFX, "NA", "SUCCEEDED","Succeeded ingesting PI data");
    return "0";  
  $$;
 
---------------------------------------------------------------------------------------------------------------------
--modify table to add new cols
alter table public.d_pi_tables add column ingest_type varchar(255);
alter table public.d_pi_tables add column key_string varchar(255); 

alter table public.d_pi_tables_history add column ingest_type varchar(255);
alter table public.d_pi_tables_history add column key_string varchar(255);    
  
  
create or replace view PUREINSIGHTS_DEV.RAW.PI_TABLES_CHANGE_DATA as
select table_name, s3_sourced, active, ingest_type, key_string, start_time, end_time, current_flag, 'I' as dml_type
from (select table_name, s3_sourced, active, ingest_type, key_string, update_timestamp as start_time,
      lag(update_timestamp) over (partition by table_name order by update_timestamp desc) as end_time_raw,
      case when end_time_raw is null then 
'9999-12-31'::timestamp_ntz else end_time_raw end as end_time,
      case when end_time_raw is null then 1 else 0 end as current_flag
      from (select table_name, s3_sourced, active,ingest_type, key_string, update_timestamp
            from pi_tables_stream
            where metadata$action = 'INSERT'
            and metadata$isupdate = 'FALSE'))
union
select table_name, s3_sourced, active,  ingest_type, key_string, start_time, end_time, current_flag, dml_type
from (select table_name, s3_sourced, active,  ingest_type, key_string, update_timestamp as start_time,
      lag(update_timestamp) over (partition by table_name order by update_timestamp desc) as end_time_raw,
      case when end_time_raw is null then 
'9999-12-31'::timestamp_ntz else end_time_raw end as end_time,
      case when end_time_raw is null then 1 else 0 end as current_flag,dml_type
      from (-- Identify data to insert into nation_history table
        select table_name, s3_sourced, active,  ingest_type, key_string, update_timestamp, 'I' as dml_type
        from pi_tables_stream
        where metadata$action = 'INSERT'
        and metadata$isupdate = 'TRUE'
union
select table_name, null, null, null,null,start_time, 'U' as dml_type
from public.d_pi_tables_history
where table_name in (select distinct table_name 
                             from pi_tables_stream
                             where metadata$action = 'INSERT'
                             and metadata$isupdate = 'TRUE')
        and current_flag = 1))
union
select str.table_name, null, null, null,null,hist.start_time, current_timestamp()::timestamp_ntz, null, 'D'
from public.d_pi_tables_history hist
inner join pi_tables_stream str
on hist.table_name = str.table_name
where str.metadata$action = 'DELETE'
and str.metadata$isupdate = 'FALSE'
and hist.current_flag = 1;
  
  
CREATE or replace PROCEDURE update_tables_history ()
  RETURNS VARCHAR
  LANGUAGE javascript
  EXECUTE AS CALLER
  AS
  
  $$
  
var sqlCommand = `merge into public.d_pi_tables_history nh
using pi_tables_change_data m 
   on  nh.table_name = m.table_name 
   and nh.start_time = m.start_time
when matched and m.dml_type = 'U' then update
    set nh.end_time = m.end_time,
        nh.current_flag = 0
when matched and m.dml_type = 'D' then update 
    set nh.end_time = m.end_time,
        nh.current_flag = 0
when not matched and m.dml_type = 'I' then insert
           (table_name, s3_sourced, active,  ingest_type, key_string, start_time, end_time, current_flag)
    values (m.table_name, m.s3_sourced, m.active,  m.ingest_type, m.key_string, m.start_time, m.end_time, m.current_flag);`;

snowflake.execute ({sqlText: sqlCommand});

sqlCommand = `update public.d_pi_tables_history hist
set hist.end_time = temp.real_end_time
from 
(select table_name, start_time, end_time ,
    lead (start_time) over (partition by table_name order by start_time asc) as proposed_end_time,
    case when proposed_end_time is not null and timestampdiff(seconds,end_time, proposed_end_time) <=60 then proposed_end_time else end_time end as real_end_time
from public.d_pi_tables_history) as temp
where hist.table_name = temp.table_name
and hist.start_time = temp.start_time
and hist.end_time != temp.real_end_time;
--and some time within 1 minute of now`;

snowflake.execute ({sqlText: sqlCommand});

return 0;

$$;
  
  update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId', update_timestamp = current_timestamp()
  where table_name in ('conversations', 'audio_quality'); 
  
 update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('transcription_topic_phrases', 'transcription_topic_configuration','transcription_program_topic_configuration',
                       'transcription_program_configuration','transcription_program_queue_mapping','transcription_program_topic_mapping');  

  
  -----------
  --group one (conversation based)
    update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId;sessionId;edgeId;codec;', update_timestamp = current_timestamp()
  where table_name in ('audio_quality');--note: raw table has no sessionId and edgeId is capt as so
  
    update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId;sessionId;segmentIndex', update_timestamp = current_timestamp()
  where table_name in ('conversations_detail'); -- no segmentIndex in parquet
  
    update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId', update_timestamp = current_timestamp()
  where table_name in ('conversations');
  
      update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId;attemptTime', update_timestamp = current_timestamp()
  where table_name in ('dialer_detail');
  
      update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId;sessionId;attemptStartTime;attemptEndTime', update_timestamp = current_timestamp()
  where table_name in ('dialer_preview_detail');
  
      update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId;divisionId', update_timestamp = current_timestamp()
  where table_name in ('divisions');
  
      update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId;sessionId;outcomeId', update_timestamp = current_timestamp()
  where table_name in ('flow_outcomes'); 
  
      update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'participantid', update_timestamp = current_timestamp()
  where table_name in ('participants');--dd has only participantId as primary key --> really? parquet has participantid
  
  --scored_agents (in Kevin spreadhsheet but not in dd)???
  
      update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId;sessionId;segmentIndex', update_timestamp = current_timestamp()
  where table_name in ('segments');
  
      update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationid;sessionId;requestedRoutingSkillId', update_timestamp = current_timestamp()
  where table_name in ('session_requested_routing_skills');--check capitalization of conversationid
  
       update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId;sessionId', update_timestamp = current_timestamp()
  where table_name in ('session_summary');
  
        update public.d_pi_tables 
  set ingest_type = 'merge_no_deletes', key_string = 'conversationId;sessionId', update_timestamp = current_timestamp()
  where table_name in ('sessions');
  
  --group two (config like files)
        update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('contact_center_settings');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('configuration_objects');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('groups_membership');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('locations');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('user_details');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('user_locations');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('external_contacts');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('user_skills');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('user_locations');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('user_roles');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('queues_membership');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('wrapup_mapping');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('queue_configuration');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('transcription_program_configuration');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('transcription_program_flow_mapping');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('transcription_program_queue_mapping');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('transcription_program_topic_mapping');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('transcription_topic_configuration');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('transcription_topic_phrases');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('wfm_activity_codes');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('wfm_management_unit_configuration');
          update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('survey_forms');
            update public.d_pi_tables 
  set ingest_type = 'truncate_insert', key_string = 'NA', update_timestamp = current_timestamp()
  where table_name in ('evaluation_forms');